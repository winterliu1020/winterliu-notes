Spark RDD是弹性分布式数据集，这些数据是分布式存储的，基于内存运算。

### 1. 单个RDD运算

![](https://winterliublog.oss-cn-beijing.aliyuncs.com/winterliu-notes/研究生学习任务/20201015151924.png)

![](https://winterliublog.oss-cn-beijing.aliyuncs.com/winterliu-notes/研究生学习任务/20201015152042.png)

![](https://winterliublog.oss-cn-beijing.aliyuncs.com/winterliu-notes/研究生学习任务/20201015152134.png)

![](https://winterliublog.oss-cn-beijing.aliyuncs.com/winterliu-notes/研究生学习任务/20201015152239.png)

### 2.多个RDD“转换”运算

![](https://winterliublog.oss-cn-beijing.aliyuncs.com/winterliu-notes/研究生学习任务/20201015152606.png)

### 3. 基本动作运算

intRDD.stats() #描述性统计信息 机器学习完成推理性统计信息的获取

![](https://winterliublog.oss-cn-beijing.aliyuncs.com/winterliu-notes/研究生学习任务/20201015152802.png)

### 4. RDD key_value "转换"运算 （上面的都是基于列表的，从这里开始基于键值对）

![](https://winterliublog.oss-cn-beijing.aliyuncs.com/winterliu-notes/研究生学习任务/20201015153209.png)

mapValues：针对值进行映射

![](https://winterliublog.oss-cn-beijing.aliyuncs.com/winterliu-notes/研究生学习任务/20201015153433.png)

reduceByKey(lambda x, y: x+y).collect();  它会按照key将它们汇聚到一起，再执行x+y操作（如果只有一项，后面的y就没有了）

### 5. 多个RDD key_value "转换"运算

RDD中（3，4）这种括号包住的是一个元组，虽然也是key value形式(key, value)，但是这里允许多个key重复，和Python中的字典不一样。

![](https://winterliublog.oss-cn-beijing.aliyuncs.com/winterliu-notes/研究生学习任务/20201015154455.png)

### 6. Key value 动作运算

![](https://winterliublog.oss-cn-beijing.aliyuncs.com/winterliu-notes/研究生学习任务/20201015154923.png)

```python
kvRDD1.lookup(5) # 对于key-value类型的RDD，该函数可以取出相同的key的value值，组成一个集合队列
r1 = kvRDD1.lookup(5)      
print(type(r1))    
<class 'list'>
```

### 7. 弹性分布式数据 （广播变量：是执行任务的多台计算机进程共享数据对的方式，分布式共享）

![](https://winterliublog.oss-cn-beijing.aliyuncs.com/winterliu-notes/研究生学习任务/20201015155547.png)

这里就把fruitMap这个字典转成了一个广播变量bcFruitMap;

因为这里是分布式的，分布式的话是在多台机器上做计算，所以得通过广播变量，因为广播变量是写到sc里面去的，写到了这个上下文环境，而n台机器是通过sc去分配任务的，那么它只要在同一个环境当中，这个广播变量就可以共享。多台数据要通信的话，是要通过这个广播变量来实现的。

![](https://winterliublog.oss-cn-beijing.aliyuncs.com/winterliu-notes/研究生学习任务/20201015160510.png)

### 8. accumulator累加器 (分布式累加)

注意它是分布式的，所以不像单机的累加；这里累加也要在sc中声明一个累加器

![](https://winterliublog.oss-cn-beijing.aliyuncs.com/winterliu-notes/研究生学习任务/20201015161126.png)

### 9. RDD Persistence持久化

将数据缓存在内存，提高性能的措施，分布式缓存。

![](https://winterliublog.oss-cn-beijing.aliyuncs.com/winterliu-notes/研究生学习任务/20201015161335.png)

![](https://winterliublog.oss-cn-beijing.aliyuncs.com/winterliu-notes/研究生学习任务/20201015161426.png)

### 10. WordCount程序

spark中主要就是map和reduce函数：map用于分解任务，reduce用于合并、归约

![](https://winterliublog.oss-cn-beijing.aliyuncs.com/winterliu-notes/研究生学习任务/20201015162448.png)

### 11. Spark数据处理，RDD 转 DataFrame

![](https://winterliublog.oss-cn-beijing.aliyuncs.com/winterliu-notes/研究生学习任务/20201015164536.png)

![](https://winterliublog.oss-cn-beijing.aliyuncs.com/winterliu-notes/研究生学习任务/20201015164905.png)

  **3.将RDD转成DataFrame (DataFrame其实是spark.sql模块上的)**

![](https://winterliublog.oss-cn-beijing.aliyuncs.com/winterliu-notes/研究生学习任务/20201015165510.png)

![](https://winterliublog.oss-cn-beijing.aliyuncs.com/winterliu-notes/研究生学习任务/20201015165626.png)

![](https://winterliublog.oss-cn-beijing.aliyuncs.com/winterliu-notes/研究生学习任务/20201015170718.png)



### 13. spark获取部分列

![](https://winterliublog.oss-cn-beijing.aliyuncs.com/winterliu-notes/研究生学习任务/20201015170758.png)

### 14. 增加列、过滤列

![](https://winterliublog.oss-cn-beijing.aliyuncs.com/winterliu-notes/研究生学习任务/20201015172102.png)

![](https://winterliublog.oss-cn-beijing.aliyuncs.com/winterliu-notes/研究生学习任务/20201015172139.png)

### 15.单个字段排序

![](https://winterliublog.oss-cn-beijing.aliyuncs.com/winterliu-notes/研究生学习任务/20201015172430.png)

![](https://winterliublog.oss-cn-beijing.aliyuncs.com/winterliu-notes/研究生学习任务/20201015172504.png)

**RDD也可以用ROW对象构造成一个dataframe对象。然后dataframe可以注册成临时表，然后用类似sql语句可以去操作。**

### 16. 多字段排序

![](https://winterliublog.oss-cn-beijing.aliyuncs.com/winterliu-notes/研究生学习任务/20201015173239.png)

![](https://winterliublog.oss-cn-beijing.aliyuncs.com/winterliu-notes/研究生学习任务/20201015173323.png)

### 17. 显示不重复数据

![](https://winterliublog.oss-cn-beijing.aliyuncs.com/winterliu-notes/研究生学习任务/20201015173653.png)

![](https://winterliublog.oss-cn-beijing.aliyuncs.com/winterliu-notes/研究生学习任务/20201015173818.png)

### 18. 预处理邮编数据集

**分组数据统计：**

![](https://winterliublog.oss-cn-beijing.aliyuncs.com/winterliu-notes/研究生学习任务/20201015174308.png)

![](https://winterliublog.oss-cn-beijing.aliyuncs.com/winterliu-notes/研究生学习任务/20201015174141.png)

![](https://winterliublog.oss-cn-beijing.aliyuncs.com/winterliu-notes/研究生学习任务/20201015174431.png)



**Join关联数据**

邮编数据处理：过滤头部列名所在行

把包含字符串的引号去掉（这里要用转义符\")，然后再根据逗号分割，返回一个二维数组（一个列表）。

![](https://winterliublog.oss-cn-beijing.aliyuncs.com/winterliu-notes/研究生学习任务/20201015174735.png)

### 19. 建立spark临时表

1.先将RDD转成dataframe

![](https://winterliublog.oss-cn-beijing.aliyuncs.com/winterliu-notes/研究生学习任务/20201015175511.png)

2.再将dataframe注册成临时表：

注意这里三种格式的数据：RDD，df, 数据库中一张表

![](https://winterliublog.oss-cn-beijing.aliyuncs.com/winterliu-notes/研究生学习任务/20201015180155.png)

![](https://winterliublog.oss-cn-beijing.aliyuncs.com/winterliu-notes/研究生学习任务/20201015180319.png)

然后将zipcode_table 和 user_table通过u.zipcode = z.zipcode连接起来：

![](https://winterliublog.oss-cn-beijing.aliyuncs.com/winterliu-notes/研究生学习任务/20201016094637.png)

### 20. pyspark转化成pandas做可视化

![](https://winterliublog.oss-cn-beijing.aliyuncs.com/winterliu-notes/研究生学习任务/20201016102217.png)

![](https://winterliublog.oss-cn-beijing.aliyuncs.com/winterliu-notes/研究生学习任务/20201016102614.png)

然后可以将列向量转成行向量：

![](https://winterliublog.oss-cn-beijing.aliyuncs.com/winterliu-notes/研究生学习任务/20201016102801.png)

![](https://winterliublog.oss-cn-beijing.aliyuncs.com/winterliu-notes/研究生学习任务/20201016103132.png)

绘制饼图：

![](https://winterliublog.oss-cn-beijing.aliyuncs.com/winterliu-notes/研究生学习任务/20201016103446.png)

